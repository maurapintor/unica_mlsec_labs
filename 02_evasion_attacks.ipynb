{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": ""
    }
   },
   "source": [
    "# Evasion attacks against Machine Learning models\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/maurapintor/unica_mlsec_labs/blob/HEAD/02_evasion_attacks.ipynb)\n",
    "\n",
    "As seen in class, machine learning models can be fooled by *adversarial examples*, samples artificially crafted to redirect the output of the victim towards a desired result.\n",
    "\n",
    "These attacks can be either:\n",
    "\n",
    "* **targeted**, in which the attacker wants to produce a specific misclassification (e.g. a dog must be recognized as a cat); or\n",
    "* **untargeted**, in which the attacker is satisfied with producing a generic misclassification (e.g. a dog will be recognized as anything else but a dog).\n",
    "\n",
    "Both targeted and untargeted attacks are formulated as optimization problems.\n",
    "\n",
    "### Targeted attacks\n",
    "\n",
    "Targeted attacks that can be written as:\n",
    "\n",
    "$$\n",
    "  \\min_\\boldsymbol{\\delta} L(\\boldsymbol{x} + \\boldsymbol{\\delta}, y_t; \\boldsymbol{\\theta})\n",
    "  \\\\\n",
    "  s.t.\\quad ||\\delta||_p \\le \\epsilon\n",
    "  \\\\\n",
    "  \\text{subject to} \\quad \\boldsymbol{l}_b \\preccurlyeq \\boldsymbol{x} + \\boldsymbol{\\delta} \\preccurlyeq \\boldsymbol{l}_u\n",
    "$$\n",
    "\n",
    "where $L$ is the objective function of our attack (it defines the goal of the attacker, *i.e.*, where to find adversarial examples), $\\boldsymbol{x}$ is the sample to perturb, $y_t$ is the target label, $\\boldsymbol{\\theta}$ are the parameters of the model, $\\epsilon$ is the maximum allowed perturbation, and $\\boldsymbol{l}_b,\\boldsymbol{l}_u$ are the input-space bounds (for instance, images must be clipped in 0-1 or 0-255 to be valid samples).\n",
    "\n",
    "\n",
    "### Untargeted attacks\n",
    "\n",
    "Untargeted attacks can be formulated as:\n",
    "\n",
    "$$\n",
    "  \\max_\\boldsymbol{\\delta} L(\\boldsymbol{x} + \\boldsymbol{\\delta}, y; \\boldsymbol{\\theta})\n",
    "  \\\\\n",
    "  s.t.\\quad ||\\delta||_p \\le \\epsilon\n",
    "  \\\\\n",
    "  \\text{subject to} \\quad \\boldsymbol{l}_b \\preccurlyeq \\boldsymbol{x} + \\boldsymbol{\\delta} \\preccurlyeq \\boldsymbol{l}_u\n",
    "$$\n",
    "\n",
    "where we change the minimization to a *maximization*, since we want to maximize the error of the classifier w.r.t. the real label $y$.\n",
    "\n",
    "We start by implementing *untargeted* evasion attacks, and we need to define two main components: the *optimization algorithm* and the *loss function* of the attack. While the second one can be *any* distance function, we will now describe one particular optimizer.\n",
    "\n",
    "In this exercise, we will leverage the *projected gradient descent* [1,2] optimizer, by implementing it step by step in SecML.\n",
    "\n",
    "First, we create a simple 2D dataset that we will use in this tutorial, and we fit an SVM classifier on top of it.\n",
    "\n",
    "[1] Biggio et al. \"Evasion attacks against machine learning at test time\", ECML PKDD 2013, https://arxiv.org/abs/1708.06131\n",
    "[2] Madry et al. \"Towards deep learning models resistant to adversarial attacks\", ICLR 2018, https://arxiv.org/pdf/1706.06083.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import secml\n",
    "    import foolbox\n",
    "except ImportError:\n",
    "    %pip install secml\n",
    "    %pip install foolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 999\n",
    "\n",
    "n_features = 2  # number of features\n",
    "n_samples = 1250  # number of samples\n",
    "centers = [[-2, 0], [2, -2], [2, 2]]  # centers of the clusters\n",
    "cluster_std = 0.8  # standard deviation of the clusters\n",
    "n_classes = len(centers)  # number of classes\n",
    "\n",
    "from secml.data.loader import CDLRandomBlobs\n",
    "\n",
    "dataset = CDLRandomBlobs(n_features=n_features,\n",
    "                         centers=centers,\n",
    "                         cluster_std=cluster_std,\n",
    "                         n_samples=n_samples,\n",
    "                         random_state=random_state).load()\n",
    "\n",
    "n_tr = 1000  # number of training set samples\n",
    "n_ts = 250  # number of test set samples\n",
    "\n",
    "# split in training and test\n",
    "from secml.data.splitter import CTrainTestSplit\n",
    "\n",
    "splitter = CTrainTestSplit(\n",
    "    train_size=n_tr, test_size=n_ts, random_state=random_state)\n",
    "tr, ts = splitter.split(dataset)\n",
    "\n",
    "# normalize the data\n",
    "from secml.ml.features import CNormalizerMinMax\n",
    "\n",
    "nmz = CNormalizerMinMax()\n",
    "tr.X = nmz.fit_transform(tr.X)\n",
    "ts.X = nmz.transform(ts.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from secml.ml.classifiers import CClassifierPyTorch\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "\n",
    "\n",
    "# creation of the multiclass classifier\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# torch model creation\n",
    "net = Net(n_features=n_features, n_classes=n_classes, n_hidden=100)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(),\n",
    "                      lr=0.001, momentum=0.9)\n",
    "\n",
    "# wrap torch model in CClassifierPyTorch class\n",
    "clf = CClassifierPyTorch(model=net,\n",
    "                         loss=criterion,\n",
    "                         optimizer=optimizer,\n",
    "                         input_shape=(n_features,),\n",
    "                         random_state=random_state)\n",
    "\n",
    "# fit the classifier\n",
    "clf.fit(tr.X, tr.Y)\n",
    "\n",
    "# compute predictions on a test set\n",
    "y_pred = clf.predict(ts.X)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "metric = CMetricAccuracy()\n",
    "acc = metric.performance_score(y_true=ts.Y, y_pred=y_pred)\n",
    "\n",
    "print(\"Accuracy on test set: {:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# plot the samples and the decision function of the classifier\n",
    "from secml.figure import CFigure\n",
    "\n",
    "fig = CFigure()\n",
    "fig.sp.plot_ds(tr)\n",
    "fig.sp.plot_decision_regions(clf, plot_background=True,\n",
    "                             n_grid_points=200)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projected Gradient Descent (PGD)\n",
    "\n",
    "The attack is formulated as follows:\n",
    "\n",
    "First, the attack is initialized by chosing a starting point for the descent, by also specifying the maximum perturbation budget $\\epsilon$, the step-size $\\alpha$, and the number of iterations.\n",
    "At each iteration, the strategy computes the gradient of the model, and it updates the adversarial example by following the computed direction.\n",
    "Lastly, if the applied perturbation is more than the intended perturbation budget $\\varepsilon$, the algorithm projects this sample back inside a valid $\\ell_p$-ball centered on the starting point, with radius $\\varepsilon$.\n",
    "\n",
    "We use an $\\ell_2$ perturbation here, for limiting the maximum Euclidean distance of the perturbed point from the original point to $\\varepsilon$.\n",
    "\n",
    "The PGD attack uses the [cross-entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) for creating adversarial examples.\n",
    "\n",
    "Recalling from the previous definitions, the objectives can be customized for targeted and untargeted attacks.\n",
    "\n",
    "* Targeted attack: minimize CE Loss on the target class (makes the model classify the sample as $y_t$)\n",
    "* Untargeted attack: maximize the CE Loss on the original class (makes the model not classify the sample as $y$)\n",
    "\n",
    "A graphical explanation of the projected gradient descent is shown below.\n",
    "\n",
    "![PGD-algorithm](assets/pgd_algorithm.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.ml.classifiers.loss import CLossCrossEntropy\n",
    "from secml.array import CArray\n",
    "\n",
    "\n",
    "def pgd_l2_untargeted(x, y, clf, eps, alpha, steps):\n",
    "    \n",
    "    # we need the gradient of the softmax\n",
    "    clf.softmax_outputs = True\n",
    "    \n",
    "    loss_func = CLossCrossEntropy()\n",
    "    x_adv = x.deepcopy()  # makes a copy of the original sample\n",
    "\n",
    "    # we use a CArray to store intermediate results\n",
    "    path = CArray.zeros((steps + 1, x.shape[1]))\n",
    "    path[0, :] = x_adv  # store initial point\n",
    "\n",
    "    for i in range(steps):\n",
    "        scores = clf.decision_function(x_adv)\n",
    "\n",
    "        # gradient of the loss w.r.t. the clf logits\n",
    "        # TODO write your code here\n",
    "        loss_gradient = ...\n",
    "        # gradient of the clf logits w.r.t. the input\n",
    "        # TODO write your code here\n",
    "        clf_gradient = ...\n",
    "        # chain rule\n",
    "        # TODO write your code here\n",
    "        gradient = ...\n",
    "\n",
    "        # normalize the gradient (takes only the direction and discards the magnitude)\n",
    "        # avoid division by 0\n",
    "        if gradient.norm() != 0:\n",
    "            # TODO write your code here\n",
    "            gradient = ...\n",
    "\n",
    "        # make step\n",
    "        # TODO write your code here\n",
    "        x_adv = ...\n",
    "\n",
    "        # project inside epsilon-ball\n",
    "        delta = x_adv - x\n",
    "        if (delta).norm() > eps:\n",
    "            # TODO write your code here\n",
    "            delta = ...\n",
    "            # recompute x_adv with the new delta\n",
    "            # TODO write your code here\n",
    "            x_adv = ...\n",
    "\n",
    "        # force input bounds\n",
    "        # TODO write your code here\n",
    "        x_adv = ...\n",
    "\n",
    "        # store point in the path\n",
    "        path[i + 1, :] = x_adv\n",
    "    \n",
    "    # restore the classifier as it was\n",
    "    clf.softmax_outputs = False\n",
    "    return x_adv, clf.predict(x_adv), path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "point = ts[index, :]\n",
    "x0, y0 = point.X, point.Y\n",
    "steps = 200\n",
    "eps = 0.3\n",
    "alpha = 0.1\n",
    "\n",
    "print(f\"Starting point has label: {y0.item()}\")\n",
    "x_adv, y_adv, attack_path = pgd_l2_untargeted(x0, y0, clf, eps, alpha, steps)\n",
    "print(f\"Adversarial point has label: {y_adv.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from secml.figure import CFigure\n",
    "from secml.optim.constraints import CConstraintL2, CConstraintBox\n",
    "\n",
    "def loss_fn(x):\n",
    "    scores = clf.decision_function(x)\n",
    "    return - CLossCrossEntropy().loss(y_true=y0, score=scores)\n",
    "    \n",
    "\n",
    "fig = CFigure(height=5, width=6)\n",
    "fig.sp.plot_fun(loss_fn, n_grid_points=200)\n",
    "\n",
    "# maximum perturbation allowed\n",
    "constraint = CConstraintL2(center=x0, radius=eps)\n",
    "fig.sp.plot_path(attack_path)\n",
    "fig.sp.plot_constraint(constraint)\n",
    "\n",
    "# feature bounds\n",
    "input_bounds = CConstraintBox(lb=0.0, ub=1.0)\n",
    "fig.sp.plot_constraint(input_bounds)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evasion achieved!\n",
    "As you could see, the process is not bug-free, and it is complex to handle.\n",
    "Hence, SecML provides a lot of attack wrappers to accomplish the same task effortlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.adv.attacks.evasion import CFoolboxPGDL2\n",
    "\n",
    "y_target = None\n",
    "lb = tr.X.min()  # lower bound of the input space\n",
    "ub = tr.X.max()  # upper bound of the input space\n",
    "\n",
    "# create attack with the Foolbox wrapped class\n",
    "# TODO write your code here\n",
    "pgd_attack = ...\n",
    "\n",
    "# run the attack\n",
    "# TODO write your code here\n",
    "y_pred, _, adv_ds_pgd, _ = ...\n",
    "\n",
    "print(\"Original x0 label: \", y0.item())\n",
    "print(\"Adversarial example label (PGD-L2): \", y_pred.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# let's plot again\n",
    "fig = CFigure(height=5, width=6)\n",
    "fig.sp.plot_decision_regions(clf, plot_background=False, n_grid_points=200)\n",
    "\n",
    "# let's plot the objective function of the attack\n",
    "# TODO write your code here\n",
    "\n",
    "constraint = CConstraintL2(center=x0, radius=eps)\n",
    "\n",
    "# optimization path of the attack\n",
    "# TODO write your code here\n",
    "\n",
    "fig.sp.plot_constraint(constraint)\n",
    "input_bounds = CConstraintBox(lb=0.0, ub=1.0)\n",
    "fig.sp.plot_constraint(input_bounds)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use the tutorial above and [the first tutorial](01_intro_secml.ipynb) to create evasion attacks against a Deep Neural Network trained on the MNIST classifier.\n",
    "\n",
    "1. Load the MNIST dataset and create a Neural Network with PyTorch.\n",
    "2. Train the model on the MNIST dataset with SecML.\n",
    "3. Use the PGD-LInf attack with $\\varepsilon=0.3$, $\\alpha=0.01$ and 200 steps to create an adversarial digit.\n",
    "4. (optional) Use the utility function used before to show the original and perturbed digit along with their predictions.\n",
    "5. Check out the other attacks available in [SecML](https://secml.readthedocs.io/)!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO write your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
