{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "5f8TaVzxiPaz"
      },
      "source": [
        "# Explaining Machine Learning\n",
        "\n",
        "Interpretability of Machine Learning models has recently become a relevant\n",
        " research direction to more thoroughly address and mitigate the issues of\n",
        " adversarial examples and to better understand the potential flaws of the\n",
        " most recent algorithm such as Deep Neural Networks.\n",
        "\n",
        "In this tutorial, we explore different methods to compute\n",
        " *post-hoc* explanations, which consist of analyzing a trained model to\n",
        " understand which components such as features or training prototypes are\n",
        " more relevant during the decision (classification) phase.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
        "https://colab.research.google.com/github/maurapintor/unica_mlsec_labs/blob/HEAD/06_explainability.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QKzFfOIiPa0"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr --no-display\n",
        "# NBVAL_IGNORE_OUTPUT\n",
        "\n",
        "%pip install secml --upgrade\n",
        "%pip install foolbox\n",
        "%pip install transformers\n",
        "%pip install shap\n",
        "%pip install datasets\n",
        "\n",
        "import secml\n",
        "import foolbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KQHmoiziPa1"
      },
      "source": [
        "## Feature-based explanations\n",
        "\n",
        "Feature-based explanation methods assign a value to each feature of an input\n",
        " sample depending on how relevant it is towards the classification\n",
        " decision. These relevance values are often called *attributions*.\n",
        "\n",
        "We start with the following *gradient-based* white-box explanation methods, applied on the image classification domain:\n",
        "\n",
        " - **Gradient**\n",
        "\n",
        " Compute gradients of the output class w.r.t. the input.\n",
        "\n",
        " > [[baehrens2010explain]](http://www.jmlr.org/papers/volume11/baehrens10a/baehrens10a.pdf)\n",
        " > D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen,\n",
        " > K.-R.Muller, \"How to explain individual classification decisions\",\n",
        " > in: J. Mach. Learn. Res. 11 (2010) 1803-1831\n",
        "\n",
        " - **Gradient * Input**\n",
        "\n",
        " Decomposes the output on a specific input by backpropagating the contributions of all neurons to every feature, providing a linear approximation.\n",
        "\n",
        " > [[shrikumar2016not]](https://arxiv.org/pdf/1605.01713)\n",
        " > A. Shrikumar, P. Greenside, A. Shcherbina, A. Kundaje,\n",
        " > \"Not just a blackbox: Learning important features through propagating\n",
        " > activation differences\", 2016 arXiv:1605.01713.\n",
        "\n",
        " > [[melis2018explaining]](https://arxiv.org/abs/1803.03544)\n",
        " > M. Melis, D. Maiorca, B. Biggio, G. Giacinto and F. Roli,\n",
        " > \"Explaining Black-box Android Malware Detection,\" 2018 26th European\n",
        " > Signal Processing Conference (EUSIPCO), Rome, 2018, pp. 524-528.\n",
        "\n",
        " - **Integrated Gradients**\n",
        "\n",
        "  Improves the linear approximation by integrating the modelâ€™s output along the path between the input and a reference sample.\n",
        "\n",
        "  > [[sundararajan2017axiomatic]](https://arxiv.org/pdf/1703.01365)\n",
        "  > Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. \"Axiomatic Attribution\n",
        "  > for Deep Networks.\" Proceedings of the 34th International Conference on\n",
        "  > Machine Learning, Volume 70, JMLR. org, 2017, pp. 3319-3328."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggAJZtu0iPa2"
      },
      "source": [
        "### Training of the classifier\n",
        "\n",
        "First, we load the MNIST dataset and we train an SVM classifier with RBF kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "CRqptQHJiPa2"
      },
      "outputs": [],
      "source": [
        "random_state = 999\n",
        "\n",
        "n_tr = 500 # number of training set samples\n",
        "n_ts = 500 # number of test set samples\n",
        "\n",
        "# load data\n",
        "from secml.data.loader import CDataLoaderMNIST\n",
        "\n",
        "loader = CDataLoaderMNIST()\n",
        "tr = loader.load(\"training\", num_samples=n_tr)\n",
        "ts = loader.load(\"testing\", num_samples=n_ts)\n",
        "\n",
        "# normalize the data between 0 and 1\n",
        "tr.X /= 255\n",
        "ts.X /= 255\n",
        "\n",
        "# creation of the multiclass classifier with RBF kernel\n",
        "from secml.ml.classifiers import CClassifierSVM\n",
        "from secml.ml.kernels import CKernelRBF\n",
        "\n",
        "clf = CClassifierSVM(kernel=CKernelRBF(gamma=1e-2))\n",
        "\n",
        "# fit the classifier\n",
        "clf.fit(tr.X, tr.Y)\n",
        "\n",
        "# compute predictions on a test set\n",
        "y_pred = clf.predict(ts.X)\n",
        "\n",
        "# evaluate the accuracy of the classifier\n",
        "from secml.ml.peval.metrics import CMetricAccuracy\n",
        "\n",
        "metric = CMetricAccuracy()\n",
        "acc = metric.performance_score(y_true=ts.Y, y_pred=y_pred)\n",
        "print(f\"Accuracy on test set: {acc:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHdwsVfXiPa2"
      },
      "source": [
        "### Compute the explanations\n",
        "\n",
        "The `secml.explanation` package provides different explanation methods\n",
        " as subclasses of `CExplainer`. Each explainer requires as input a\n",
        " trained classifier.\n",
        "\n",
        "To compute the explanation on a sample, the `.explain()` method should be used.\n",
        "For *gradient-based* methods, the label `y` of the class w.r.t. the explanation\n",
        " should be computed is required.\n",
        "\n",
        "The `.explain()` method will return the relevance value associated to each\n",
        " feature of the input sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YllpBVcbiPa3"
      },
      "outputs": [],
      "source": [
        "from secml.explanation import \\\n",
        "    CExplainerGradient, CExplainerGradientInput, CExplainerIntegratedGradients\n",
        "\n",
        "# TODO write your code here\n",
        "# initialize the explainers\n",
        "explainers = {\n",
        "    \"gradient\": ...,\n",
        "    \"gradient * input\": ...,\n",
        "    \"integrated gradients\": ...\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LI4LF-5iPa3"
      },
      "outputs": [],
      "source": [
        "i = 123  # test sample on which explanations should be computed\n",
        "x, y = ts[i, :].X, ts[i, :].Y\n",
        "\n",
        "print(f\"Explanations for sample {i} (true class: {y.item()})\")\n",
        "\n",
        "from secml.array import CArray\n",
        "\n",
        "# dictionary where to collect attributions of different methods\n",
        "attributions = {}\n",
        "\n",
        "for expl_name in explainers:\n",
        "\n",
        "  # compute explanations (attributions) wrt each class\n",
        "  print(f\"Computing explanations using '{expl_name}'...\")\n",
        "\n",
        "  # empty array where to collect attributions values\n",
        "  attr = CArray.empty(shape=(tr.num_classes, x.size))\n",
        "\n",
        "  # loop over classes...\n",
        "  for c in tr.classes:\n",
        "    # TODO write your code here\n",
        "    attr_c = ... # compute the explanation\n",
        "    attr[c, :] = attr_c\n",
        "\n",
        "  attributions[expl_name] = attr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyJTgjTGiPa3"
      },
      "source": [
        "### Visualize results\n",
        "\n",
        "We now visualize the explanations computed using the different methods, in rows.\n",
        "In columns, we show the explanations w.r.t. each different class.\n",
        "\n",
        "Above the original tested sample, its true class label is shown.\n",
        "\n",
        "Red (blue) pixels denote positive (negative) relevance of the corresponding\n",
        " feature wrt the specific class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8qx6N2HiPa3"
      },
      "outputs": [],
      "source": [
        "from secml.figure import CFigure\n",
        "# only required for visualization in notebooks\n",
        "%matplotlib inline\n",
        "\n",
        "fig = CFigure(height=4.5, width=14, fontsize=13)\n",
        "\n",
        "for i, expl_name in enumerate(explainers):\n",
        "\n",
        "  attr = attributions[expl_name]\n",
        "\n",
        "  sp_idx = i * (tr.num_classes+1)\n",
        "\n",
        "  # original image\n",
        "  fig.subplot(len(explainers), tr.num_classes+1, sp_idx+1)\n",
        "  fig.sp.imshow(x.reshape((tr.header.img_h, tr.header.img_w)), cmap=\"gray\")\n",
        "\n",
        "  if i == 0:  # For the first row only\n",
        "      fig.sp.title(f\"Origin y: {y.item()}\")\n",
        "\n",
        "  fig.sp.ylabel(expl_name)  # label of the explainer\n",
        "\n",
        "  fig.sp.yticks([])\n",
        "  fig.sp.xticks([])\n",
        "\n",
        "  # threshold to plot positive and negative relevance values symmetrically\n",
        "  th = max(abs(attr.min()), abs(attr.max()))\n",
        "\n",
        "  # plot explanations\n",
        "  for c in tr.classes:\n",
        "\n",
        "    fig.subplot(len(explainers), tr.num_classes+1, sp_idx+2+c)\n",
        "    fig.sp.imshow(attr[c, :].reshape((tr.header.img_h, tr.header.img_w)),\n",
        "                  cmap=\"seismic\", vmin=-1*th, vmax=th)\n",
        "\n",
        "    fig.sp.yticks([])\n",
        "    fig.sp.xticks([])\n",
        "\n",
        "    if i == 0:  # for the first row only\n",
        "      fig.sp.title(\"c: {:}\".format(c))\n",
        "\n",
        "fig.title(\"Explanations\", x=0.55)\n",
        "fig.tight_layout(rect=[0, 0.003, 1, 0.94])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--ecvmPyiPa3"
      },
      "source": [
        "For both **gradient\\*input** and **integrated gradients** methods we can\n",
        " observe a well defined area of positive (red) relevance for the explanation\n",
        " computed w.r.t. the digit 6. This is expected as the true class of the tested\n",
        " sample is in fact 6. Moreover, a non-zero relevance value is mainly assigned\n",
        " to the features which are present in the tested sample, which is an expected\n",
        " behavior of these explanation methods.\n",
        "\n",
        "Conversely, the **gradient** method assigns relevance to a wider area of the\n",
        " image, even external to the actual digit. This leads to explanations which\n",
        " are in many cases difficult to interpret. For this reason, more advanced\n",
        " explanation methods are often favored."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg0YUrxHiPa4"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Compute the explanations of adversarial examples.\n",
        "Use the PGD L-inf attack, with eps=0.3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmLlze2ciPa4"
      },
      "outputs": [],
      "source": [
        "from secml.adv.attacks.evasion import CFoolboxPGDLinf\n",
        "\n",
        "y_target  = 2\n",
        "attack = CFoolboxPGDLinf(clf, y_target=y_target,\n",
        "                         lb=0.0, ub=1.0, steps=30,\n",
        "                         epsilons=0.3, abs_stepsize=0.01,\n",
        "                         random_start=False)\n",
        "\n",
        "_, _, adv_ds, _ = attack.run(x, y)\n",
        "\n",
        "pred = clf.predict(adv_ds.X).item()\n",
        "print(pred)\n",
        "\n",
        "# TODO write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxwXQFYoiPa4"
      },
      "source": [
        "## Prototype-based explanation\n",
        "\n",
        "Prototype-based explanation methods identify the most responsible training\n",
        " points for a given prediction to explain the behavior of machine learning models.\n",
        "\n",
        "In this tutorial, we are going to test the explanation method proposed in:\n",
        "\n",
        "  > [[koh2017understanding]](https://arxiv.org/pdf/1703.04730)\n",
        "  > Koh, Pang Wei, and Percy Liang, \"Understanding black-box predictions\n",
        "  > via influence functions\", in: Proceedings of the 34th International\n",
        "  > Conference on Machine Learning-Volume 70. JMLR. org, 2017.\n",
        "\n",
        "It selects training samples through influence functions, that estimate how the\n",
        " modelâ€™s predictions change without a certain training sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q54eKzWaiPa4"
      },
      "source": [
        "### Training the classifier\n",
        "\n",
        "As our implementation of the prototype-based explanation methods currently\n",
        " only supports binary classifiers, we load the 2-classes MNIST59 dataset and\n",
        " then we train a SVM classifier with RBF kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbkxVQ2FiPa4"
      },
      "outputs": [],
      "source": [
        "n_tr = 100 # number of training set samples\n",
        "n_ts = 500 # number of test set samples\n",
        "\n",
        "digits = (5, 9) # we load only samples from those classes\n",
        "\n",
        "# load data\n",
        "loader = CDataLoaderMNIST()\n",
        "tr = loader.load(\"training\", digits=digits, num_samples=n_tr)\n",
        "ts = loader.load(\"testing\", digits=digits, num_samples=n_ts)\n",
        "\n",
        "# normalize the data between 0 and 1\n",
        "tr.X /= 255\n",
        "ts.X /= 255\n",
        "\n",
        "# creation of the multiclass classifier with RBF kernel\n",
        "clf = CClassifierSVM(kernel=CKernelRBF(gamma=1e-2))\n",
        "\n",
        "# fit the classifier\n",
        "print(\"Training of classifier...\")\n",
        "clf.fit(tr.X, tr.Y)\n",
        "\n",
        "# compute predictions on a test set\n",
        "y_pred = clf.predict(ts.X)\n",
        "\n",
        "# evaluate the accuracy of the classifier\n",
        "metric = CMetricAccuracy()\n",
        "acc = metric.performance_score(y_true=ts.Y, y_pred=y_pred)\n",
        "print(\"Accuracy on test set: {:.2%}\".format(acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF_OUq2DiPa4"
      },
      "source": [
        "### Compute the influential training prototypes\n",
        "\n",
        "The `CExplainerInfluenceFunctions` class provides the influence functions\n",
        " prototype-based method described previously. It requires as input the\n",
        " classifier to explain and its training set.\n",
        "It also requires the identifier of the loss used to train the classier. In the\n",
        " case of SVM, it is the `hinge` loss.\n",
        "\n",
        "To compute the influence of each training sample wrt the test samples,\n",
        " the `.explain()` method should be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "JCXx1wZAiPa5"
      },
      "outputs": [],
      "source": [
        "from secml.explanation import CExplainerInfluenceFunctions\n",
        "\n",
        "# TODO write your code here\n",
        "# initialize the explainer\n",
        "explanation = ...\n",
        "\n",
        "print(\"Computing influence of each training prototype on test samples...\")\n",
        "\n",
        "# TODO write your code here\n",
        "# compute the explanations\n",
        "infl = ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tD3KozAiPa5"
      },
      "source": [
        "### Visualize results\n",
        "\n",
        "We now visualize, wrt each class, the 3 most influential training prototypes\n",
        " for two different test samples. Above each training sample, the influence\n",
        " value is shown.\n",
        "\n",
        "In addition, above the original tested samples, the true class label is shown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THDjqziriPa5"
      },
      "outputs": [],
      "source": [
        "fig = CFigure(height=3.5, width=9, fontsize=13)\n",
        "\n",
        "n_xc = 3  # number of training prototypes to plot per class\n",
        "\n",
        "ts_list = (50, 100)  # test samples to evaluate\n",
        "\n",
        "infl_argsort = infl.argsort(axis=1)  # sort influence values\n",
        "\n",
        "for i, ts_idx in enumerate(ts_list):\n",
        "\n",
        "    sp_idx = i * (n_xc*tr.num_classes+1)\n",
        "\n",
        "    x, y = ts[ts_idx, :].X, ts[ts_idx, :].Y\n",
        "\n",
        "    pred = clf.predict(x)\n",
        "\n",
        "    # original image\n",
        "    fig.subplot(len(ts_list), n_xc*tr.num_classes+1, sp_idx+1)\n",
        "    fig.sp.imshow(x.reshape((tr.header.img_h, tr.header.img_w)), cmap=\"gray\")\n",
        "\n",
        "    fig.sp.title(f\"Orig: {ts.header.y_original[y.item()]} Pred: {ts.header.y_original[pred.item()]}\")\n",
        "\n",
        "    fig.sp.yticks([])\n",
        "    fig.sp.xticks([])\n",
        "\n",
        "    tr_top = infl_argsort[ts_idx, :n_xc]\n",
        "    tr_top = tr_top.append(infl_argsort[ts_idx, -n_xc:])\n",
        "\n",
        "    # plot top influential training prototypes\n",
        "    for j, tr_idx in enumerate(tr_top[::-1]):  # sort highest first\n",
        "        fig.subplot(len(ts_list), n_xc*tr.num_classes+1, sp_idx+2+j)\n",
        "        fig.sp.imshow(tr.X[tr_idx, :].reshape((tr.header.img_h, tr.header.img_w)), cmap=\"gray\")\n",
        "\n",
        "        fig.sp.title(f\"{infl[ts_idx, tr_idx].item():.2f}\")\n",
        "\n",
        "        fig.sp.yticks([])\n",
        "        fig.sp.xticks([])\n",
        "\n",
        "fig.title(\"Top influential training prototypes\", x=0.57)\n",
        "fig.tight_layout()\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mfgF2seiPa5"
      },
      "source": [
        "For both the tested samples we can observe a direct correspondence between\n",
        " the most influencial training prototypes and their true class. Specifically,\n",
        " the samples having highest (lowest) influence values are (are not) from\n",
        " the same true class of the tested samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfRlcHe9iPa5"
      },
      "source": [
        "## Exercise 2\n",
        "Visualize the prototypes for a poisoned classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lUNbOPsiPa5"
      },
      "outputs": [],
      "source": [
        "# TODO write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explaining language models\n",
        "\n",
        "In the last part of this tutorial we switch to the Natural Language Processing domain, focusing on transformer-based architectures. In particular we consider [BERT](https://arxiv.org/abs/1810.04805), a Large Language Model (LLM) pre-trained on a large text corpus which can be quickly fine-tuned on a wide range of downstream tasks, by attaching a different layer on top of the representation space.\n",
        "\n",
        "We will rely to the [Hugging Face](https://huggingface.co) `transformers` library, which provides APIs, tools, and an open model zoo to easily download and load available models.\n",
        "\n",
        "For this tutorial we select a text classification model that tries to infer emotions, among six different classes (sadness, joy, love, anger, fear, surprise). We start loading the [pre-trained model](https://huggingface.co/nateraw/bert-base-uncased-emotion) from Hugging Face."
      ],
      "metadata": {
        "id": "no5G3fxhAyjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "\n",
        "# load the model and tokenizer\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    \"nateraw/bert-base-uncased-emotion\", use_fast=True\n",
        ")\n",
        "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"nateraw/bert-base-uncased-emotion\"\n",
        ").cuda()\n",
        "\n",
        "# build a pipeline object to do predictions\n",
        "pred = transformers.pipeline(\n",
        "    \"text-classification\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0,\n",
        "    return_all_scores=True,  # observe the modelâ€™s behavior for all classes, not just the top output\n",
        ")\n"
      ],
      "metadata": {
        "id": "cN2VK2lxA8EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SHAP\n",
        "\n",
        "To provide local explanations we use [SHAP](https://github.com/shap/shap) (SHapley Additive exPlanations), a black-box unified framework based on Shapley values, a method from cooperative game theory that assigns each feature an importance value for a particular prediction.\n",
        "\n",
        "> [[lundberg2017shap]](https://papers.nips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf) Scott M. Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17). Curran Associates Inc., Red Hook, NY, USA, 4768-4777.\n",
        "\n",
        "Among the available techniques implemented in SHAP, we leverage the `Partition` method, which computes Shapley values recursively through a hierarchy of features by masking (i.e., removing) features from the input sample during\n",
        "the computation of the explanation and evaluating the model outputs, capturing the correlation between input features.\n",
        "\n",
        "SHAP fully support Hugging Face transformers library, so that we only have to pass the classifier pipeline to the explainer."
      ],
      "metadata": {
        "id": "UGQ2Ci3UC30E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "\n",
        "# write a sentence\n",
        "data = [\"I am not happy, I'm sad rather than happy\"]\n",
        "\n",
        "# we first classify it\n",
        "print(pred(data)[0])\n",
        "\n",
        "# TODO write your code here\n",
        "# initialize the explainer\n",
        "explainer = ...\n",
        "\n",
        "# TODO write your code here\n",
        "# compute the attributions\n",
        "shap_values = ...\n"
      ],
      "metadata": {
        "id": "KXw5ESYhC5Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize force plots\n",
        "\n",
        "Once the SHAP values are computed, we can visualize feature attributions towards individual classes through force plots.\n",
        "\n",
        "The base value is what the model outputs when the entire input text is masked, while $f_{class}(inputs)$ is the output of the model for the full original input. The SHAP values explain in an additive way how the impact of unmasking each word changes the model output from the base value (where the entire input is masked) to the final prediction value."
      ],
      "metadata": {
        "id": "9DSoox3oDq5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO write your code here\n"
      ],
      "metadata": {
        "id": "45pV0Y5pDqE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global explanations\n",
        "\n",
        "If we compute the local explanations on a set of samples and aggregate the results, we can obtain the most relevant features that influence the behavior of the model on that dataset.\n",
        "\n",
        "We first load an emotion dataset from Hugging Face and randomly pick 10 samples, then compute SHAP values on them."
      ],
      "metadata": {
        "id": "mMl9TrHbFBF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# load the emotion dataset\n",
        "dataset = datasets.load_dataset(\"emotion\", split=\"train\")\n",
        "data = pd.DataFrame({\"text\": dataset[\"text\"], \"emotion\": dataset[\"label\"]})\n",
        "\n",
        "# TODO write your code here\n",
        "# get attributions for ten random samples\n",
        "shap_values = ...\n"
      ],
      "metadata": {
        "id": "lfx9mRNCE2QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize results\n",
        "\n",
        "We can plot the most relevant features that impact a single class through a bar chart, after averaging the attributions for the selected class."
      ],
      "metadata": {
        "id": "xgX8TQbGGVJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "red_shap_values = shap_values[:, :, \"joy\"].mean(0)\n",
        "\n",
        "# TODO write your code here"
      ],
      "metadata": {
        "id": "U9moRrA3G-sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can sort the bar chart in decending order\n",
        "shap.plots.bar(red_shap_values, order=shap.Explanation.argsort)"
      ],
      "metadata": {
        "id": "qHBhokw1HK8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ...or acending order\n",
        "shap.plots.bar(red_shap_values, order=shap.Explanation.argsort.flip)\n"
      ],
      "metadata": {
        "id": "IftTxy87H2fr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "6829dbcfe73f7e6ba320fd39e7c4bddd23e92d1a15475ecfb0305a1647487c5f"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}